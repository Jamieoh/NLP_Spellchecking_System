{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled10.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "2b_tK3ew97PL",
        "colab_type": "code",
        "outputId": "dc602009-0451-4f60-c9cf-a8fb4e6095e4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 235
        }
      },
      "cell_type": "code",
      "source": [
        "# Import relevant libraries\n",
        "import os\n",
        "import nltk\n",
        "import re\n",
        "from statistics import mode\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import sent_tokenize\n",
        "import itertools\n",
        "import json\n",
        "from nltk.tag.stanford import StanfordNERTagger\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "lmtzr = WordNetLemmatizer()\n",
        "from difflib import SequenceMatcher\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package maxent_ne_chunker to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
            "[nltk_data] Downloading package words to /root/nltk_data...\n",
            "[nltk_data]   Package words is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 114
        }
      ]
    },
    {
      "metadata": {
        "id": "AIQObBMtC_b9",
        "colab_type": "code",
        "outputId": "995f608e-c9de-4384-dead-d7676dd87108",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "dOq9TwmQAe5s",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# because i'm using 'utf-8' i have problems with chunking\n",
        "inputfile='/content/gdrive/My Drive/football_players.txt'\n",
        "buf = open(inputfile, encoding=\"UTF-8\")\n",
        "list_of_doc = buf.read().split('\\n')\n",
        "\n",
        "l = []\n",
        "for i in list_of_doc:\n",
        "    if len(i) !=0:\n",
        "        l.append(i)\n",
        "list_of_doc = l\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kdt9Zaz5LeyN",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**Task 1**"
      ]
    },
    {
      "metadata": {
        "id": "rqNtAO0-DItF",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# This function takes each document and performs: 1) sentence segmentation 2) tokenization 3) part-of-speech tagging\n",
        "\n",
        "def ie_preprocess(document):\n",
        "  \n",
        "  pos_sentences = []\n",
        " \n",
        "  for document in list_of_doc:\n",
        "\n",
        "    # The following performs Sentence Segmentation\n",
        "    pos_sentences = nltk.sent_tokenize(document) \n",
        "    \n",
        "    # The following performs tokenization\n",
        "    pos_sentences = [nltk.word_tokenize(sent) for sent in pos_sentences]  \n",
        "    \n",
        "    # The following performs part-of-speech tagging\n",
        "    pos_sentences = [nltk.pos_tag(sent) for sent in pos_sentences]  \n",
        "    \n",
        "    # Returns pos_sentences\n",
        "    return pos_sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "WYMxlAQNDP9D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "outputId": "24ea043e-3fe0-4439-f74d-f764441605f5"
      },
      "cell_type": "code",
      "source": [
        "# The following code checks the result for the first document\n",
        "\n",
        "first_doc=list_of_doc[0]\n",
        "# Calling the ie_preprocess function\n",
        "pos_sent = ie_preprocess(first_doc)\n",
        "\n",
        "# Display all tagged sentences as given in the assignment sheet. However, to dissplay all tagged sentence, please remove the \n",
        "# list index\n",
        "pos_sent[2]"
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('In', 'IN'),\n",
              " ('2008', 'CD'),\n",
              " (',', ','),\n",
              " ('he', 'PRP'),\n",
              " ('won', 'VBD'),\n",
              " ('his', 'PRP$'),\n",
              " ('first', 'JJ'),\n",
              " ('Ballon', 'NNP'),\n",
              " (\"d'Or\", 'NN'),\n",
              " ('and', 'CC'),\n",
              " ('FIFA', 'NNP'),\n",
              " ('World', 'NNP'),\n",
              " ('Player', 'NNP'),\n",
              " ('of', 'IN'),\n",
              " ('the', 'DT'),\n",
              " ('Year', 'NN'),\n",
              " ('awards', 'NNS'),\n",
              " ('.', '.')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 131
        }
      ]
    },
    {
      "metadata": {
        "id": "6YeCetimLiQc",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**Task 2**"
      ]
    },
    {
      "metadata": {
        "id": "IwTtjYFI3U7E",
        "colab_type": "code",
        "outputId": "2bb5aa6a-163f-444e-90ad-4e5ae2727117",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 163
        }
      },
      "cell_type": "code",
      "source": [
        "# This function takes the list of tokens with POS tags for each sentence and returns the named entities (NE).\n",
        "# NLTK provides a classier that has already been trained to recognize named entities, accesse the function _nltk.ne_chunk()_. \n",
        "# If we set the parameter binary=True, then named entities are j tagged as NE; otherwise, the classier adds category labels such as \n",
        "# PERSON, ORGANIZATION etc.\n",
        "\n",
        "def named_entity_finding(pos_sent, x=True, y=\"NE\"):\n",
        "    \n",
        "    named_entities = []\n",
        "        \n",
        "    # The code below identifies all the named entities like person, place, organisation etc.  \n",
        "      \n",
        "    tree = nltk.ne_chunk(pos_sent, binary=x)\n",
        "\n",
        "    for subtree in tree.subtrees():\n",
        "        if subtree.label() == y:\n",
        "            entity = \"\"\n",
        "            for leaf in subtree.leaves():\n",
        "                entity = entity + leaf[0] + \" \"\n",
        "            named_entities.append(entity.strip())\n",
        "\n",
        "    return named_entities\n",
        "    \n",
        "        \n",
        "# Calls the function above - ie_preprocess\n",
        "pos_sents=ie_preprocess(list_of_doc[0])\n",
        "# Test\n",
        "named_entity_finding(pos_sents[0])"
      ],
      "execution_count": 132,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Cristiano Ronaldo',\n",
              " 'Santos Aveiro',\n",
              " 'ComM',\n",
              " 'GOIH',\n",
              " 'Portuguese',\n",
              " 'Spanish',\n",
              " 'Real Madrid',\n",
              " 'Portugal']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 132
        }
      ]
    },
    {
      "metadata": {
        "id": "zZngmyRob4mL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**Task 3**"
      ]
    },
    {
      "metadata": {
        "id": "_ukI3nT9woce",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 181
        },
        "outputId": "641510d6-34d5-4f87-d3b1-fa748f7ae883"
      },
      "cell_type": "code",
      "source": [
        "# Here i use the named_entity_finding() function to extract all NEs for each document.\n",
        "# I extract named entities for the whole document and flatten it to a list at the end.\n",
        "def NE_flat_list_fn(document):\n",
        "  \n",
        "  named_entities=[]\n",
        "  \n",
        "  # Perform loop through all the documents in question\n",
        "  for i in document:\n",
        "    # POS tag the docs\n",
        "    pos_sents = ie_preprocess(i)\n",
        "    \n",
        "    for pos_sent in pos_sents:\n",
        "      \n",
        "      #Find the named entity for every tagged sentence in the doc.\n",
        "      entity = named_entity_finding(pos_sent)\n",
        "      if len(entity) != 0:\n",
        "        named_entities.append(entity)\n",
        "        \n",
        "        # Flatten the list\n",
        "        NE_flat_list = list(itertools.chain.from_iterable(named_entities))\n",
        "  # Return flat list     \n",
        "  return NE_flat_list\n",
        "\n",
        "# Test\n",
        "# 20 named entites displayed\n",
        "ne_flat = NE_flat_list_fn(list_of_doc)\n",
        "list(set(ne_flat))[1:10]"
      ],
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Ballon',\n",
              " 'Portuguese Football Federation',\n",
              " 'FIFA Club',\n",
              " 'Ronaldo',\n",
              " 'FIFA Ballon',\n",
              " 'Cristiano Ronaldo',\n",
              " 'UEFA European',\n",
              " 'Portugal',\n",
              " 'Silver Boot']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 134
        }
      ]
    },
    {
      "metadata": {
        "id": "CZvfplie8h9k",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**Task 4**"
      ]
    },
    {
      "metadata": {
        "id": "lQ9BNGA65Iiy",
        "colab_type": "code",
        "outputId": "2161c61a-18a5-4ca9-95da-5e9840111396",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# The following function extracts the players name\n",
        "\n",
        "def name_of_the_player(doc):\n",
        "    \n",
        "    name = []\n",
        "    \n",
        "    # Calls ie_preprocess function\n",
        "    pos_sents = ie_preprocess(doc)\n",
        "    \n",
        "    # Returns the best chunk tokens for given structure\n",
        "    tree = nltk.ne_chunk(pos_sents[0])\n",
        "\n",
        "    # Returns tree along with joining players name\n",
        "    # Ensures is person not organisation etc\n",
        "    for subtree in tree.subtrees():\n",
        "        if subtree.label() == \"PERSON\":\n",
        "            entity = \"\"\n",
        "            for leaf in subtree.leaves():\n",
        "                entity = entity + leaf[0] + \" \"\n",
        "            name.append(entity.strip())\n",
        "    return \" \".join(name)\n",
        "  \n",
        "# Test\n",
        "print(name_of_the_player(list_of_doc[0]))"
      ],
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cristiano Santos Aveiro\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "7DwiFZbwCDpB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "28194318-83c6-48b1-e2c8-ea216c18c6c2"
      },
      "cell_type": "code",
      "source": [
        "# The following function extracts the team of the player\n",
        "\n",
        "def team_of_the_player(doc): \n",
        "    \n",
        "    # Identiy following patterns\n",
        "    patterns = [\n",
        "        'for .*? club (.*?) (and|after)',\n",
        "        'captains both (.*?) (and|after)',\n",
        "        'the\\s(.*?)\\snational (team)'\n",
        "    ]\n",
        "    possible_teams = []\n",
        "    # Loop to generate list of teams.\n",
        "    for p in patterns:\n",
        "        if re.findall(p, doc, re.IGNORECASE):\n",
        "            for m in re.findall(p, doc, re.IGNORECASE):\n",
        "                possible_teams.append(m)\n",
        "\n",
        "    filterteams = []\n",
        "    \n",
        "    # Loop generated to extrct team. Ensures length is greater than 3 and no full stops included.\n",
        "    for t in possible_teams:\n",
        "        if len(t[0]) < 25 and len(t[0]) > 3 and '.' not in t[0]:\n",
        "            filterteams.append(t[0])\n",
        "    # Return filtered set of teams        \n",
        "    return set(filterteams)\n",
        "  \n",
        "# Test  \n",
        "print(team_of_the_player(list_of_doc[0]))"
      ],
      "execution_count": 154,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'Real Madrid', 'Portugal'}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "Sch8CXCRBf-O",
        "colab_type": "code",
        "outputId": "1c4955b7-b1ac-43c8-d938-abc9f1841895",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# The following function extracts the players position\n",
        "\n",
        "def position_of_the_player(doc):\n",
        "\n",
        "    # List of all possible football positions\n",
        "    positions = [\n",
        "        \"Goalkeeper\",\n",
        "        \"Centre-back\",\n",
        "        \"Sweeper\",\n",
        "        \"Full-back\",\n",
        "        \"Wing-back\",\n",
        "        \"Centre midfield\",\n",
        "        \"Defensive midfield\",\n",
        "        \"Attacking midfield\",\n",
        "        \"Wide midfield\",\n",
        "        \"Centre forward\",\n",
        "        \"Second striker\",\n",
        "        \"Winger\",\n",
        "        \"Forward\"\n",
        "    ]\n",
        "    player_position = []\n",
        "    \n",
        "    sent = sent_tokenize(doc)\n",
        "    for i, sent in enumerate(sent):\n",
        "        for x in positions:\n",
        "            # Find matching in look up and sentence.\n",
        "            regex = re.compile(r'\\b({0})\\b'.format(x), flags=re.IGNORECASE)\n",
        "                \n",
        "            # if result is true append to a list for testing at the end.\n",
        "            r = bool(regex.search(sent))\n",
        "                \n",
        "            if r == True:\n",
        "                player_position.append(x)\n",
        "    # Returns list of players positions   \n",
        "    return list(set(player_position))\n",
        "\n",
        "# Prints the position of the following player in the doc\n",
        "print(position_of_the_player(list_of_doc[0]))"
      ],
      "execution_count": 155,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Forward']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "-G9nq3jOFWCO",
        "colab_type": "code",
        "outputId": "f21673eb-7689-4293-8491-e146dda7ddf7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# The following function extracts the players date of birth\n",
        "\n",
        "def date_of_birth(doc):\n",
        "    \n",
        "    \"\"\"Firstly I used regex to find the date of birth followed by extracting the particular \n",
        "    sentence that contained the word born as this is significant to date of birth\"\"\"\n",
        "    sentence = sent_tokenize(doc)[0]\n",
        "    \n",
        "    # Find matching\n",
        "    match = re.compile(r'born\\b\\s*((?:\\S+\\s+){0,3})')\n",
        "    \n",
        "    # Search for occurrences in the patern anywhere in the string\n",
        "    extract_born = match.findall(sentence)[0]\n",
        "    \n",
        "    # Replace pattern with the search above\n",
        "    extract_born = re.sub('\\W+',' ', extract_born )\n",
        "    \n",
        "    # Returns date of birth\n",
        "    return extract_born\n",
        "\n",
        "# Test\n",
        "print(date_of_birth(list_of_doc[0]))"
      ],
      "execution_count": 161,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "5 February 1985 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "rZbk6wfFG-x5",
        "colab_type": "code",
        "outputId": "2a277639-7c13-4972-c0c3-f1d3e9524171",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# The following function extracts the players country of origin\n",
        "\n",
        "def country_of_origin(doc):\n",
        "  \n",
        "    # I extracted the following pattern so that the players country of origin is directly before the word professional.\n",
        "    # The code is as follows.   \n",
        "    pattern = 'is\\san?\\s(.*?)\\sprofessional'\n",
        "    \n",
        "    # Searches for occurrences in the pattern anywhere in the document\n",
        "    return re.findall(pattern, doc)[0]\n",
        "\n",
        "# Test\n",
        "print(country_of_origin(list_of_doc[0]))"
      ],
      "execution_count": 162,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Portuguese\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "sy5SIzc5wJS-",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**Task 5**"
      ]
    },
    {
      "metadata": {
        "id": "0K5ZtJaZIDtp",
        "colab_type": "code",
        "outputId": "ea248295-a704-4bef-e23f-2720de062f3f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "cell_type": "code",
      "source": [
        "# The following function uses the outputs from the previous functions by calling it for generation into JSON-LD output\n",
        "\n",
        "def data_generator(doc):\n",
        "    \n",
        "    # Calls all the functions above\n",
        "    data = [name_of_the_player(doc), date_of_birth(doc), country_of_origin(doc), position_of_the_player(doc), team_of_the_player(doc)]\n",
        "    \n",
        "    # Returns the data\n",
        "    return data\n",
        "\n",
        "data = data_generator(list_of_doc[0])\n",
        "# Test\n",
        "print(data)"
      ],
      "execution_count": 163,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Cristiano Santos Aveiro', '5 February 1985 ', 'Portuguese', ['Forward'], {'Real Madrid', 'Portugal'}]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "U6xB5DUDJEKb",
        "colab_type": "code",
        "outputId": "6579b58e-c5a2-4bb7-bd1c-b71afdfcedd9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        }
      },
      "cell_type": "code",
      "source": [
        "# The following function uses the output from the previous functions to create JSON-LD output\n",
        "\n",
        "def generate_jsonld1(arg, con=True):\n",
        "    if con == True:\n",
        "\n",
        "        ld = { \"@id\": \"http://my-soccer-ontology.com/footballer/\"+arg[0],\n",
        "\n",
        "            \"name\": arg[0],\n",
        "            \"born\": arg[1],\n",
        "            \"country\": arg[2],\n",
        "            \"position\": [\n",
        "                { \"@id\": \"http://my-soccer-ontology.com/position/\",\n",
        "                    \"type\": arg[3]\n",
        "                }\n",
        "             ],   \n",
        "             \"team\": [\n",
        "                { \"@id\": \"http://my-soccer-ontology.com/team/\",\n",
        "                    \"name\": arg[4]\n",
        "                }   \n",
        "             ]\n",
        "        }\n",
        "\n",
        "        return json.dumps(ld)\n",
        "    \n",
        "    elif con == False:\n",
        "        \n",
        "        ld = { \"@id\": \"http://my-soccer-ontology.com/footballer/\"+arg[0],\n",
        "\n",
        "            \"name\": arg[0],\n",
        "            \"born\": arg[1],\n",
        "            \"country\": arg[2],\n",
        "            \"position\": [\n",
        "                { \"@id\": \"http://my-soccer-ontology.com/position\",\n",
        "                    \"type\": arg[3]\n",
        "                }\n",
        "             ],   \n",
        "             \"team\": [\n",
        "                { \"@id\": \"http://my-soccer-ontology.com/team\",\n",
        "                    \"name\": arg[4]\n",
        "                },\n",
        "             ],\n",
        "            \"Debut Year\": arg[5][0],\n",
        "            \"Debut Age\": arg[5][1]\n",
        "        }\n",
        "\n",
        "        return json.dumps(ld)\n",
        "        \n",
        "try:\n",
        "    data = data_generator(list_of_doc[0])\n",
        "    print(generate_jsonld1(data))\n",
        "except:\n",
        "    print(\"Please make sure that list index given is right\")"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{\"@id\": \"http://my-soccer-ontology.com/footballer/Cristiano Santos Aveiro\", \"name\": \"Cristiano Santos Aveiro\", \"born\": \"5 February 1985 \", \"country\": \"Portuguese\", \"position\": [{\"@id\": \"http://my-soccer-ontology.com/position/\", \"type\": [\"Forward\"]}], \"team\": [{\"@id\": \"http://my-soccer-ontology.com/team/\", \"name\": [\"Portugal national team\", \"Real Madrid\"]}]}\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "GvjAxByZKscl",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "##**Task 6**"
      ]
    },
    {
      "metadata": {
        "id": "JW3Yzg0FKrWx",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "d9b7fa77-2350-4439-b5f0-7ce48707c136"
      },
      "cell_type": "code",
      "source": [
        "# The following function extracts the debut year and age of the player\n",
        "\n",
        "def debut_age_relation(doc):\n",
        "\n",
        "    sent = sent_tokenize(doc)\n",
        "    # Debut\n",
        "    debut = []\n",
        "    # Age\n",
        "    age = []\n",
        "    \n",
        "    # For each sentence check if the debut key is present or not\n",
        "    for se in sent:\n",
        "        sp_sent = se.split()\n",
        "        \n",
        "         # Check for debut and age in the one sentence \n",
        "        if \"debut\" in sp_sent and \"aged\" in sp_sent:\n",
        "            # Check if age is present by two digit number\n",
        "            age = re.findall('\\d{2}', \" \".join(sp_sent))\n",
        "            age.append(age[0]) \n",
        "        \n",
        "        if \"debut\" in sp_sent:\n",
        "            # Check if debut year is present by a four digit number\n",
        "            date = re.findall('\\d{4}', \" \".join(sp_sent))\n",
        "            if len(date) !=0:\n",
        "                debut.append(date[0])\n",
        "        \n",
        "    # Returns debut and age\n",
        "    return [debut[0], age[0]]\n",
        "\n",
        "# Test\n",
        "debut_age_relation(list_of_doc[4])"
      ],
      "execution_count": 166,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['2002', '20']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 166
        }
      ]
    },
    {
      "metadata": {
        "id": "iuZow6tl5zI6",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Add above function to data_generator function \n",
        "\n",
        "def data_generator(doc):\n",
        "    \n",
        "    data = [name_of_the_player(doc), date_of_birth(doc), country_of_origin(doc), position_of_the_player(doc), team_of_the_player(doc), relation_debutYearAge(doc)]\n",
        "    return data\n",
        "\n",
        "data = data_generator(list_of_doc[0])\n",
        "print(data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "RCSmDQK9DoFA",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Generate JSON \n",
        "\n",
        "generate_jsonld1(data, False)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}