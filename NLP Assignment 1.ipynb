{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLP_18_19_Assignment_12458152_jamieohalloran.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "metadata": {
        "colab_type": "text",
        "id": "2oP1uun77cIh"
      },
      "cell_type": "markdown",
      "source": [
        "# Assignment 1\n",
        "\n",
        "This assignment will involve the creation of a spellchecking system and an evaluation of its performance. You may use the code snippets provided in Python for completing this or you may use the programming language or environment of your choice\n",
        "\n",
        "Please start by downloading the corpus `holbrook.txt` from Blackboard\n",
        "\n",
        "The file consists of lines of text, with one sentence per line. Errors in the line are marked with a `|` as follows\n",
        "\n",
        "    My siter|sister go|goes to Tonbury .\n",
        "    \n",
        "In this case the word 'siter' was corrected to 'sister' and the word 'go' was corrected to 'goes'.\n",
        "\n",
        "In some places in the corpus two words maybe corrected to a single word or one word to a multiple words. This is denoted in the data using underscores e.g.,\n",
        "\n",
        "    My Mum goes out some_times|sometimes .\n",
        "    \n",
        "For the purpose of this assignment you do not need to separate these words, but instead you may treat them like a single token.\n",
        "\n",
        "*Note: you may use any functions from NLTK to complete the assignment. It should not be necessary to use other libraries and so please consult with us if your solution involves any other external library. If you use any function from NLTK in Task 6 please include a brief description of this function and how it contributes to your solution.*"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "TIVCSJV-7kDs"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 1 (10 Marks)\n",
        "\n",
        "Write a parser that can read all the lines of the file `holbrook.txt` and print out for each line the original (misspelled) text, the corrected text and the indexes of any changes. The indexes refers to the index of the words in the sentence. In the example given, there is only an error in the 10th word and so the list of indexes is [9]. It is not necessary to analyze where the error occurs inside the word.\n",
        "\n",
        "Then split your data into a test set of 100 lines and a training set."
      ]
    },
    {
      "metadata": {
        "id": "btkx7ODTQB3b",
        "colab_type": "code",
        "outputId": "9dcb340c-68fc-49df-9080-f6b2a2171b76",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "cell_type": "code",
      "source": [
        "# import all libraries needed for this task\n",
        "import pandas as pd\n",
        "import re\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "from tqdm import tqdm\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "metadata": {
        "id": "PQrqbUaWQaxu",
        "colab_type": "code",
        "outputId": "ee93ca76-09b1-4c8f-e97b-18624eca67f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "# Mount from google drive\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "RznCZ1mw7mfk",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Open the holbrook text file. \n",
        "sentences = open('/content/gdrive/My Drive/holbrook.txt').readlines()\n",
        "\n",
        "# Parsing the sentences to original and corrected.\n",
        "list_original=[]\n",
        "list_corrected=[]\n",
        "list_indexes=[]\n",
        "\n",
        "for i in sentences:\n",
        "    words=word_tokenize(i)\n",
        "    ind=0\n",
        "    original_data=[]\n",
        "    corrected_data=[]\n",
        "    indexes=[]\n",
        "    \n",
        "    for word in words:\n",
        "        if(re.match(r'\\w+[|](\\w+)',word)):\n",
        "            \n",
        "            # Next word beside '\\w+[|](\\w+)' is stored in the corrected_data list.\n",
        "            corrected_data.append(re.split(r'\\w+[|](\\w+)',word)[1])\n",
        "            # The word previous to '\\w+[|](\\w+)' is stored in the original_data list.\n",
        "            original_data.append(re.split(r'(\\w+)[|]\\w+',word)[1])\n",
        "            # The index of the error in question        \n",
        "            indexes.append(ind)\n",
        "            \n",
        "        else:\n",
        "           \n",
        "            # If there is no '\\w+[|](\\w+)' in sentence, sentence is stored in list_original and list_corrected.\n",
        "            original_data.append(word)\n",
        "            corrected_data.append(word)\n",
        "            \n",
        "        # index increases by +1 when the following if/else statement is executed.\n",
        "        ind+=1\n",
        "    \n",
        "    # the data is updated and stored in the relavant list that it's associated with.   \n",
        "    list_original.append(original_data)\n",
        "    list_corrected.append(corrected_data)\n",
        "    list_indexes.append(indexes)\n",
        "    \n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "qMrCf6iJQB3h",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Loading the list_original, list_corrected and list_indexes data to 'sentences'.\n",
        "data=[]\n",
        "for i in range(len(sentences)):\n",
        "    data.append({\"original\":list_original[i],\"corrected\":list_corrected[i],\"indexes\":list_indexes[i]})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "Z-wROfl2QB3i",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Testing\n",
        "# Prints list of all the sentences\n",
        "sentences"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "tWovND_HQB3m",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Testing.\n",
        "# Printing the data in sentence numner 2\n",
        "print(data[2])\n",
        "\n",
        "\n",
        "# Test your code with the following\n",
        "assert(data[2] == {\n",
        " 'original': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'siter', '.'],\n",
        " 'corrected': ['I', 'have', 'four', 'in', 'my', 'Family', 'Dad', 'Mum', 'and', 'sister', '.'],\n",
        " 'indexes': [9]\n",
        "})"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "eRSX4I0H7pSC"
      },
      "cell_type": "markdown",
      "source": [
        "The counts and assertions given in the following sections are based on splitting the training and test set as follows"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "Kt9aR2Gy7p1C",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Splitting the data to test 100 lines and remaining training lines\n",
        "test = data[:100]\n",
        "train = data[100:]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xnob0D_F2-YE",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Splitting the data to test - first 100 lines and remaining training lines\n",
        "def test_train_split():\n",
        "\n",
        "  # Splitting the data to test - first 100 lines and remaining training lines.\n",
        "    test = data[:100]\n",
        "    train = data[100:]\n",
        "    \n",
        "    # Seperating the train original, test original, test corrected and train corrected from dictionary to list.\n",
        "    train_corrected = [elem['corrected'] for elem in train]\n",
        "    tokenizer = RegexpTokenizer(r'\\w+')\n",
        "    test_corrected = [elem['corrected'] for elem in test]\n",
        "    test_original = [elem['original'] for elem in test]\n",
        "    \n",
        "    # Removing all special characters from the list.\n",
        "    test_original = [tokenizer.tokenize(\" \".join(elem)) for elem in test_original]\n",
        "    test_corrected = [tokenizer.tokenize(\" \".join(elem)) for elem in test_corrected]\n",
        "    train_corrected = [tokenizer.tokenize(\" \".join(elem)) for elem in train_corrected]\n",
        "    \n",
        "    # Returns the following\n",
        "    return test_corrected, test_original, train_corrected\n",
        "\n",
        "# Test and Training data.\n",
        "test_corrected, test_original, train_corrected = test_train_split()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "rbaSjgcQg2Qn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Testing the test_corrected data to see if it prints out correctly\n",
        "test_corrected"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "hm5JL7cH7sLK"
      },
      "cell_type": "markdown",
      "source": [
        "## **Task 2** (10 Marks): \n",
        "Calculate the frequency (number of occurrences), *ignoring case*, of all words and their unigram probability from the corrected *training* sentences.\n",
        "\n",
        "*Hint: use `Counter` to implement this so it may be called many times*"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "7ge0uHS-7uEK",
        "outputId": "a60f3a00-5d35-4c71-9e33-fccb0f196226",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# Import relevant libraries\n",
        "from collections import Counter\n",
        "\n",
        "# from sklearn.feature_extraction.text import CountVectorizer\n",
        "# The following function returns a unigram function for a given word in the train data\n",
        "def create_unigrams(train):\n",
        "    text=''\n",
        "    for i in range(0,len(train)):\n",
        "        corrected=train[i]['corrected']\n",
        "        for j in corrected:\n",
        "            text+=j+' '\n",
        "            \n",
        "    # split the texts into tokens\n",
        "    unigrams = word_tokenize(text)\n",
        "    unigrams = [token.lower() for token in unigrams if len(token) > 1] \n",
        "    \n",
        "    # the conunter is used to implement the number of occurences of all words and it may be called many times\n",
        "    # Returns the unigram and the number of occurrences of the unigram\n",
        "    unigram_freq=Counter(unigrams)\n",
        "    return unigrams,unigram_freq\n",
        "\n",
        "# Defines the function unigram for each word\n",
        "# Returns the frequency of all words and thei unigram probability from the corrected training sentences  \n",
        "def unigram(word):\n",
        "    return prob(unigram_frequency,word)\n",
        "\n",
        "# Defines the function prob for each unigram and its associated word\n",
        "# Returns the actual unigram frequency along with its associated word\n",
        "def prob(unigram_freq,word):\n",
        "    return unigram_freq[word]\n",
        "\n",
        "unigrams,unigram_frequency=create_unigrams(train)\n",
        "print(unigram('me'))\n",
        "\n",
        "\n",
        "\n",
        "#Test your code with the following\n",
        "assert(unigram(\"me\")==87)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "87\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "w8r8QYj78GPK"
      },
      "cell_type": "markdown",
      "source": [
        "## **Task 3** (15 Marks): \n",
        "[Edit distance](https://en.wikipedia.org/wiki/Edit_distance) is a method that calculates how similar two strings are to one another by counting the minimum number of operations required to transform one string into the other. There is a built-in implementation in NLTK that works as follows:\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "SV9Mu8P38IQE",
        "outputId": "d52b2f89-a856-44a5-f4ef-a7073a59bcb5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "from nltk.metrics.distance import edit_distance\n",
        "\n",
        "# Edit distance returns the number of changes to transform one word to another\n",
        "print(edit_distance(\"hello\", \"hi\"))"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "Hm46Lbiz8K8M"
      },
      "cell_type": "markdown",
      "source": [
        "Write a function that calculates all words with *minimal* edit distance to the misspelled word. You should do this as follows\n",
        "\n",
        "1. Collect the set of all unique tokens in `train`\n",
        "2. Find the minimal edit distance, that is the lowest value for the function `edit_distance` between `token` and a word in `train`\n",
        "3. Output all unique words in `train` that have this same (minimal) `edit_distance` value\n",
        "\n",
        "*Do not implement edit distance, use the built-in NLTK function `edit_distance`*"
      ]
    },
    {
      "metadata": {
        "id": "JYW_gPZVijTB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "22822ea8-6b1a-4024-e0f9-5f1d1752ce18"
      },
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "def get_candidates(token):\n",
        "  \n",
        "    #Collects the set of all unique tokens in train, 'unigrams' below is the list of all of the tokens. unq_unigrams sorts them in a list by uniqueness\n",
        "    unq_unigrams=list(set(unigrams))\n",
        "    similar_text=[]\n",
        "    \n",
        "    #Calculate the minimal edit distance, that is the lowest value for the function edit_distance between token and a word in train(unq_unigrams)\n",
        "    for i in unq_unigrams:\n",
        "        if(nltk.edit_distance(i,token)<2):\n",
        "            similar_text.append(i)\n",
        "            similar_text.sort(reverse=True)\n",
        "    \n",
        "    #Output all unique words in train(unq_unigrams) that have this same (minimal) edit_distance value\n",
        "    return similar_text\n",
        "\n",
        "print(get_candidates(\"minde\"))\n",
        "\n",
        "# Test your code as follows\n",
        "assert get_candidates(\"minde\") == ['mine', 'mind']"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['mine', 'mind']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "RGY-eCkN8TIM"
      },
      "cell_type": "markdown",
      "source": [
        "## Task 4 (15 Marks):\n",
        "\n",
        "Write a function that takes a (misspelled) sentence and returns the corrected version of that sentence. The system should scan the sentence for words that are not in the dictionary (set of unique words in the training set) and for each word that is not in the dictionary choose a word in the dictionary that has minimal edit distance and has the highest *unigram probability*. \n",
        "\n",
        "*Your solution to this should involve `get_candidates`*\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "dIGKE4_P8WGP",
        "outputId": "f5ccdeaf-98a2-4ddb-d317-444f95f3f21f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "cell_type": "code",
      "source": [
        "# This function returns the corrected sentence based on unigram probability\n",
        "def correct(sentence):\n",
        "    corrected_sentences=[]\n",
        "    \n",
        "    #For finding candidates with minimal edit distance\n",
        "    for i in sentence:\n",
        "      \n",
        "      # Calls the get_candidates function to choose a word in the dictionary that has minimal edit distance and has the highest unigram probability\n",
        "      # Calls edit_dist and unigram_prob also\n",
        "        alternatives=get_candidates(i)\n",
        "        edit_dist=[]\n",
        "        unigram_prob=[]\n",
        "        \n",
        "        # If the word has an alternative of greater than 0 then run the following function\n",
        "        if(len(alternatives)>0):\n",
        "          \n",
        "            # For each word in alternatives(get_candidates) calculate the minimal edit distance and the highest unigram probability\n",
        "            for j in alternatives:\n",
        "                unigram_prob.append(unigram(j))\n",
        "                edit_dist.append(nltk.edit_distance(i,j))\n",
        "                \n",
        "                # The following two lines of code converts the words with the minimal edit distance and has the highest unigram probability and lists them in a dictionary.\n",
        "                # zip returns a list while dict is the constructor\n",
        "            alternative_dist=dict(zip(alternatives,edit_dist))    \n",
        "            alternative_unigram=dict(zip(alternatives,unigram_prob))\n",
        "            \n",
        "            # Sorts the previous two lines of code with respect to the second element. key=lambda x[1] does this\n",
        "            alternative_dist_s=sorted(alternative_dist.items(), key=lambda x: x[1])\n",
        "            alternative_unigram_s=sorted(alternative_unigram.items(),key=lambda x: x[1],reverse=True)\n",
        "            \n",
        "            # Take the suggested word with minimum probability \n",
        "            y=min(alternative_dist_s,key=lambda item:item[1])\n",
        "            unig_out=[item for item in alternative_dist_s if y[1] in item]\n",
        "            if(len(unig_out)==1):\n",
        "                corrected_sentences.append(unig_out[0][0])\n",
        "            else:\n",
        "              \n",
        "                # Take the suggested word with maximum probability\n",
        "                y=max(alternative_unigram_s,key=lambda item:item[1])\n",
        "                unig_out=[item for item in alternative_unigram_s if y[1] in item]\n",
        "                corrected_sentences.append(unig_out[0][0])\n",
        "                \n",
        "    # Return the corrected sentence\n",
        "    return corrected_sentences\n",
        "\n",
        "# Test code as follows\n",
        "print(correct([\"this\",\"whitr\",\"cat\"]))\n",
        "assert(correct([\"this\",\"whitr\",\"cat\"])==[\"this\",\"white\",\"cat\"])"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['this', 'white', 'cat']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "oG7jC6au8kka"
      },
      "cell_type": "markdown",
      "source": [
        "##Task 5 (10 Marks): \n",
        "Using the test corpus evaluate the *accuracy* of your method, i.e., how many words from your system's output match the corrected sentence (you should count words that are already spelled correctly and not changed by the system)."
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "HSXTQypR8mdR",
        "outputId": "f17d7079-de5c-4fb1-96c6-432c55782334",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "cell_type": "code",
      "source": [
        "def accuracy(corrected, original):\n",
        "    \n",
        "    #The following two lines of code is associted with word to word accuracy calculation. It compares each individual word (original_corrected) with the actual word (corrected_l) and calculates accuracy.\n",
        "    corrected_l = corrected\n",
        "    original_corrected = correct(original)\n",
        "    \n",
        "    # if the length of the individual word in original_corrected  is the same as the ouputed corrected_l then there has been no change so the prediction is accuracte, therefore taking accuracy as+1.\n",
        "    if len(original_corrected) == 0 and len(corrected_l) == 0:\n",
        "        accuracy = 1.0\n",
        "    else:\n",
        "      # The following takes all of the original_corrected words that are the same as the corrected_l words divides it by the length of the corrected_l list. Thus, giving the accuracy.\n",
        "      accuracy = sum(1 for x,y in zip(original_corrected,corrected_l) if x == y) / len(set(corrected_l))\n",
        "    \n",
        "    return accuracy\n",
        "  \n",
        "acc = []\n",
        "for i in tqdm(range(len(test_corrected))):\n",
        "    acc.append(accuracy(test_corrected[i], test_original[i]))\n",
        "\n",
        "        \n",
        "print(\"Average Accuracy of words in each sentence: \", round(sum(acc)/len(acc)*100), \"%\")"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [01:15<00:00,  1.22it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "************************************************EVALUATION**********************************************************\n",
            "Average Accuracy of words in each sentence:  44 %\n",
            "12 out of 100 sentences predicted correctly without any error.\n",
            "Elapsed Time is:  75.95444583892822\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "9b-r2JzD8_Zh"
      },
      "cell_type": "markdown",
      "source": [
        "## **Task 6 (35 Marks):**\n",
        "\n",
        "Consider a modification to your algorithm that would improve the accuracy of the algorithm developed in Task 3 and 4\n",
        "\n",
        "* You may resources beyond those provided here.\n",
        "* You must **not use the test data** in this task.\n",
        "* Provide a short text describing what you intend to do and why. \n",
        "* Full marks for this section may be obtained without an implementation, but an implementation is preferred.\n",
        "* Your implementation should not consist of more than 50 lines of code\n",
        "\n",
        "Please note this task is marked according to: demonstration of knowledge from the lecutures (10), originality and appropriateness of solution (10), completeness of description (10) and technical correctness (5)\n",
        "\n",
        "...............................................................................................................................................................\n",
        "\n",
        "##**Modification Ideas and Implementation:**\n",
        "\n",
        "** Wordnet.words():**\n",
        "*\tThis can be implemented in order to check if the word in question is a valid word in the text corpus. It’s the most reliable way to measure the semantic similarity of two words. It’s an NLTK corpus reader that can be imported by the following line of code:\n",
        "\n",
        ">>> from nltk.corpus import wordnet\n",
        "\n",
        "\n",
        "** Lemmatisation and Stemming:**\n",
        "* These text normalisation techniques are widely used in natural language processing to prepare text, words and documents for further processing. The NLTK library contains the necessary methods to conduct lemmatisation and stemming on the training data. Stemming is the process of reducing inflection in words to their root forms. It involves linking a certain word or words to the derived stem. Stems are generated by adding or removing suffixes/prefixes such as –ed, -ize, -s, -de and mis with a word. I would add these suffixes/prefixes to the word in question to see if the word is then present in the dictionary. This would lead to the careful, precise preparation of documents for processing which can lead to increased accuracy in the spellchecking system.  \n",
        "\n",
        "** Detect tense of the words in the training data:**\n",
        "*\tThe main outcome of this is to detect the tense of the words in the training data. Firstly, the POS tags need to be obtained. According to the part-of-speech tags in the Penn treebank project, VBD and VBN refer to the past tense. VBD and VBN also refer to the past participle of a verb. A parser may also be used in order to identify the verb phrase. \n",
        "*\t\"By the time I will reach there, he'd have already left”\n",
        "*\tSentences such as the following may cause confusion and certain rules regarding POS tags must be adhered to. The auxiliary verb in the above sentence is followed by a past tense verb and is this presented as the present perfect tense.\n",
        "\n",
        "** Word derivative - most probable word:**\n",
        "*\tThe nltk_derivationally_related_forms can be implemented here. The get_candidates function can benefit greatly from this implementation as every suggested word from it can be identified, its unigram probability can then be taken to identify the most relatable word to replace the incorrect word in the sentence. It's based on language model probability and can greatly increase the accuracy of the spellchecker.\n",
        "\n",
        "** Smoothing:**\n",
        "* Smoothing is another feature than can be implemented to increase the accuracy of your spellchecker. Its objective is to basically increase the probability of matching misspelled/incorrect words with its derivative form. It greatens their probability towards unknown n-grams. Upon doing some research in deciding what's the best smoothing implementation to improve the accuracy, the interpolated modified Kneser-Ney smoothing yields the best performance. There are three ways that produce the benefits of smoothing: absolute-discounting, interpolation and word histories. Aiming to predict the probability of a word in a given context is very difficult but these three techniques aim to simplify this process leading to a more precise, accurate spellchecker. \n",
        "\n",
        "** Fuzzy Ratio:**\n",
        "* Fuzzy ratio can also be implemented for string matching and corresponding changes can be made. Fuzzy String Matching is the process of identifying strings that are similar to another string based on the similarity of their given pattern. The similarity is measured by edit distance. Edit distance is the amount of basic operations needed to convert a string into its exact/appropriate match. These basic operations include insertion of a particular character, deletion of a particular character and substitution of a particular character. Fuzzy String Matching can be implemented excellently for spellchecking NLP systems while greatly increasing accuracy ensuring an efficient model.\n",
        "\n",
        "** Example:** \n",
        "* > from fuzzywuzzy import fuzz\n",
        "* > fuzz.ratio(\"this is a test\", \"this is a test!\")\n",
        "* > 96\n",
        "\n",
        "** Recurrent Neural Network:**\n",
        "* Implementing a recurrent neural network based language model could improve the accuracy of the algorithm developed in Task 3 and 4. A neural network can order and score sentences based on the likelihood that the sentence is a valid/accurate prediction. Grammatical and semantic correctness is the main outcome here. If scoring and predicting the sentence isn’t sufficient enough then a new sentence/new piece of text can be generated. \n",
        "* The key idea in recurrent neural networks is to accurately predict the next word in the sentence. In order to do this it’s essential to know what word comes before it in the sentence in question. RNNs are called recurrent for this very reason as the neural network performs the same task on each element (word) in the sequence (sentence). The output is dependent on the previous computations. RNNs also have the ability to store and calculate valuable information that has been captured thus far. The ‘brain’ of the RNN is knowledgeable and excellent. \n",
        "* I would do the following to implement a recurrent neural network into my model. Lets say there is 7 words in a sentence. The RNN would compute this sentence into a 7 layer neural network. Each word in the sentence has its own subsequent layer. The formula that’s associated with RNNs Is then applied to each of the 7 layers of the sentence. \n",
        "\n",
        "** LSTM: Long short-term memory networks:**\n",
        "* Vanishing gradients and exploding gradients are two major problems in traditional RNNs. The LSTM model aims to resolve these issues by implementing a framework called ‘memory cell’. The memory cell consists of four core elements: an input gate, a self-recurrent connection (a connection to itself), a forget gate and an output gate. These elements act as a translator and modifies interactions between the memory cell and its environment. LSTM networks can greatly improve the accuracy of the algorithm developed in task 3 and 4.  If i were to implement this I wouldo exactly as I have just explained: implement a LSTM model so that a memory cell can create the four gates mentioned above. Each word from the text file can then be processed through the network leading to accurate predictions, increasing accuracy.\n",
        "\n",
        "* The gates contain the ability to:\n",
        "* **Input:** alter the state of the memory cell\n",
        "* **Output:** prevent the effect the memory cell has on specific neurons\n",
        "* **Forget:** refrain the memory cells recurrent connection while remembering past connections inside\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "colab_type": "text",
        "id": "GLzaC6D28sK9"
      },
      "cell_type": "markdown",
      "source": [
        "## **Task 7 (5 Marks):**\n",
        "\n",
        "Repeat the evaluation (as in Task 5) of your new algorithm and show that it outperforms the algorithm from Task 3 and 4"
      ]
    },
    {
      "metadata": {
        "colab_type": "code",
        "id": "bz9P3GyDZyBV",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "corrected_l=[]\n",
        "original_corrected=[]\n",
        "\n",
        "# Function calculating the improved accuracy of the algorithm developed in part 5. \n",
        "def accuracy(corrected_l, original_corrected):\n",
        "    \n",
        "    # Corrects the relevant list to convert to lower for efficiency puposes of the spellchecker.\n",
        "    for i in test:\n",
        "        c=0\n",
        "        for words in i['original']:\n",
        "            if(c in i['indexes']):\n",
        "                original_corrected.append(correct(words.lower()))\n",
        "            else:\n",
        "                original_corrected.append(words.lower())\n",
        "            c+=1\n",
        "        for words in i['corrected']:\n",
        "            corrected_l.append(words.lower())\n",
        "    \n",
        "    # The following takes all of the original_corrected words that are the same as the corrected_l words divides it by the length of the corrected_l list. Thus, giving the accuracy.\n",
        "    accuracy_t=sum(1 for x,y in zip(original_corrected,corrected_l) if x == y) / len(corrected_l)\n",
        "    \n",
        "    # Returns the result of the above accuracy equation.\n",
        "    return accuracy_t\n",
        "\n",
        "acc=[]\n",
        "for i in tqdm(range(len(test_corrected))):\n",
        "    acc.append(accuracy(test_corrected[i], test_original[i]))\n",
        "\n",
        "    # Prints average accuracy of words in each sentence\n",
        "print(\"Average Accuracy of words in each sentence: \", round(sum(acc)/len(acc)*100), \"%\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}